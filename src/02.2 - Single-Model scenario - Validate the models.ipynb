{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#this is to auto-reload modules (like utils.py)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import json\n",
    "import glob\n",
    "import ast\n",
    "\n",
    "import shutil\n",
    "\n",
    "import utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# config to not reduce column width\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "import plotly as ply\n",
    "import plotly.express as px\n",
    "\n",
    "from itertools import permutations, combinations\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import ComplementNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "# import warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# warnings.filterwarnings('always')\n",
    "\n",
    "# Surpress warnings:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 16\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "%matplotlib inline  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the performances obtained by GridSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Features</th>\n",
       "      <th>BalAcc</th>\n",
       "      <th>Sens</th>\n",
       "      <th>Spec</th>\n",
       "      <th>f1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Prec</th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Hyperparams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Imbalanced</td>\n",
       "      <td>All Features</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.44</td>\n",
       "      <td>SVC</td>\n",
       "      <td>{'probability': True, 'C': 3, 'class_weight': 'balanced', 'gamma': 'auto', 'kernel': 'rbf', 'random_state': 42}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NB</td>\n",
       "      <td>Imbalanced</td>\n",
       "      <td>All Features</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.36</td>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>{'alpha': 3.5, 'norm': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT</td>\n",
       "      <td>Imbalanced</td>\n",
       "      <td>All Features</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.32</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>{'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 4, 'random_state': 42}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NN</td>\n",
       "      <td>Imbalanced</td>\n",
       "      <td>All Features</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.60</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.3, 'hidden_layer_sizes': (23,), 'learning_rate': 'constant', 'learning_rate_init': 0.7, 'max_iter': 1000, 'random_state': 42, 'solver': 'sgd'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>k-NN</td>\n",
       "      <td>Imbalanced</td>\n",
       "      <td>All Features</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.27</td>\n",
       "      <td>RadiusNeighborsClassifier</td>\n",
       "      <td>{'leaf_size': 50, 'metric': 'manhattan', 'outlier_label': 1, 'radius': 1.0, 'weights': 'uniform'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Imbalanced</td>\n",
       "      <td>All Features</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.59</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>{'eval_metric': 'mlogloss', 'learning_rate': 0.5, 'max_depth': 25, 'n_estimators': 100, 'random_state': 42, 'use_label_encoder': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RF</td>\n",
       "      <td>Imbalanced</td>\n",
       "      <td>All Features</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.64</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 10, 'n_estimators': 100, 'random_state': 42}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>Imbalanced</td>\n",
       "      <td>All Features</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.72</td>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>{'depth': 10, 'gpu_ram_part': 0.8, 'iterations': 100, 'l2_leaf_reg': 3, 'learning_rate': 0.03, 'logging_level': 'Silent', 'max_bin': 32, 'random_state': 42}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model     Dataset      Features  BalAcc  Sens  Spec    f1   AUC   Acc  Prec                 Classifier                                                                                                                                                                       Hyperparams\n",
       "0       SVM  Imbalanced  All Features    0.83  0.82  0.85  0.57  0.91  0.84  0.44                        SVC                                                                   {'probability': True, 'C': 3, 'class_weight': 'balanced', 'gamma': 'auto', 'kernel': 'rbf', 'random_state': 42}\n",
       "1        NB  Imbalanced  All Features    0.79  0.78  0.79  0.49  0.84  0.79  0.36               ComplementNB                                                                                                                                                     {'alpha': 3.5, 'norm': False}\n",
       "2        DT  Imbalanced  All Features    0.78  0.83  0.74  0.46  0.85  0.75  0.32     DecisionTreeClassifier                                                                                          {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 4, 'random_state': 42}\n",
       "3        NN  Imbalanced  All Features    0.77  0.61  0.94  0.59  0.91  0.90  0.60              MLPClassifier  {'activation': 'relu', 'alpha': 0.3, 'hidden_layer_sizes': (23,), 'learning_rate': 'constant', 'learning_rate_init': 0.7, 'max_iter': 1000, 'random_state': 42, 'solver': 'sgd'}\n",
       "4      k-NN  Imbalanced  All Features    0.74  0.80  0.68  0.41  0.75  0.70  0.27  RadiusNeighborsClassifier                                                                                 {'leaf_size': 50, 'metric': 'manhattan', 'outlier_label': 1, 'radius': 1.0, 'weights': 'uniform'}\n",
       "5   XGBoost  Imbalanced  All Features    0.72  0.49  0.95  0.53  0.88  0.89  0.59              XGBClassifier                                           {'eval_metric': 'mlogloss', 'learning_rate': 0.5, 'max_depth': 25, 'n_estimators': 100, 'random_state': 42, 'use_label_encoder': False}\n",
       "6        RF  Imbalanced  All Features    0.70  0.44  0.96  0.52  0.88  0.89  0.64     RandomForestClassifier                                                                    {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 10, 'n_estimators': 100, 'random_state': 42}\n",
       "7  CatBoost  Imbalanced  All Features    0.68  0.38  0.98  0.49  0.90  0.90  0.72         CatBoostClassifier                      {'depth': 10, 'gpu_ram_part': 0.8, 'iterations': 100, 'l2_leaf_reg': 3, 'learning_rate': 0.03, 'logging_level': 'Silent', 'max_bin': 32, 'random_state': 42}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#read\n",
    "csv_file = 'exec_results/results.csv'\n",
    "df_best = utils.read_csv(csv_file)\n",
    "df_best.head(3)\n",
    "\n",
    "\n",
    "df_best = df_best.loc[(\n",
    "    (df_best.Dataset  == 'Imbalanced')\n",
    "   &(df_best.Features == 'All Features')\n",
    ")].copy()\n",
    "\n",
    "\n",
    "df_best = df_best.groupby(\n",
    "    by=['Model']\n",
    ").first().reset_index(drop=False).sort_values(\n",
    "    by=['BalAcc','Sens', 'Spec'],\n",
    "    ascending=False,\n",
    ").reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_best['Model'] = df_best.Model.apply(lambda x: utils.get_model_short_description(x))\n",
    "\n",
    "display(df_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Training, Validation, Balanced sets \n",
    "\n",
    "##### NOTE: the \"training_sets\" contains the sets of training and validation to help the models execution, with the following information:\n",
    " - `Dataset Info `  [\"Training\", \"Training Balanced\"]\n",
    " - `Features Info`  [\"All Features\", \"Feature Selection\"]\n",
    " - `\"X\" to train`   \n",
    " - `\"y\" to train` \n",
    " - `\"X\" to validation`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_train_fs, y_train, \\\n",
    "X_train_balanced, X_train_fs_balanced, y_train_balanced,\\\n",
    "X_valid, X_valid_fs, y_valid, training_sets = utils.get_train_and_validation_data(\n",
    "    return_training_sets=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary function to get X and y according to the datasets (training, balanced...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(dset_info, feat_info):\n",
    "    X_ret = None\n",
    "    y_ret = None\n",
    "    X_valid_ret = None\n",
    "    \n",
    "    for dataset_info, features_info, X, y , X_valid in training_sets: \n",
    "        if (dataset_info == dset_info) and (features_info == feat_info):\n",
    "            X_ret = X\n",
    "            y_ret = y\n",
    "            X_valid_ret = X_valid\n",
    "            \n",
    "    return X_ret, y_ret, X_valid_ret\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the best models and create instances using their hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qty. of Models: 8\n",
      "Example:\n",
      "8 samples were saved\n"
     ]
    }
   ],
   "source": [
    "df_best_by_model = df_best.copy()\n",
    "\n",
    "df_best_by_model = df_best.groupby(\n",
    "    by=['Model', 'Dataset', 'Features']#,'balanced_accuracy','sensitivity', 'specificity']\n",
    ").head(20).reset_index(drop=True).sort_values(\n",
    "    by=['BalAcc','Sens', 'Spec'],\n",
    "    ascending=False,\n",
    ").reset_index(drop=True)\n",
    "\n",
    "\n",
    "best_models = []\n",
    "\n",
    "df_best_models = None\n",
    "\n",
    "for index, row in df_best_by_model.iterrows():\n",
    "    \n",
    "    model   = row.Model\n",
    "    dataset = row.Dataset\n",
    "    features = row.Features\n",
    "    classif = row.Classifier\n",
    "    params  = row.Hyperparams\n",
    "\n",
    "    performance_gridsearch = {\n",
    "        'BalAcc': row.BalAcc,\n",
    "        'Sens': row.Sens,\n",
    "        'Spec': row.Spec,\n",
    "        'f1': row.f1,\n",
    "        'AUC': row.AUC,\n",
    "        'Acc': row.Acc,\n",
    "        'Prec': row.Prec,\n",
    "        \n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        params_dict = ast.literal_eval(params)\n",
    "    except Exception as ex:\n",
    "        print(params)\n",
    "        print(f'<<ERROR>>: {ex}')\n",
    "        print()\n",
    "    \n",
    "\n",
    "    klass = globals()[classif]\n",
    "    clf = klass(**params_dict)\n",
    "    \n",
    "    best_models.append([model, dataset, features, clf, params, performance_gridsearch])\n",
    "   \n",
    "    # store best models into dataFrame\n",
    "    data = {\n",
    "        'Model': model,\n",
    "        'Dataset': dataset,\n",
    "        'Features': features,\n",
    "        \n",
    "        'BalAcc': row.BalAcc,\n",
    "        'Sens': row.Sens,\n",
    "        'Spec': row.Spec,\n",
    "        'f1': row.f1,\n",
    "        'AUC': row.AUC,\n",
    "        'Acc': row.Acc,\n",
    "        'Prec': row.Prec,\n",
    "        'Classifier': classif,\n",
    "        'Hyperparams': params,\n",
    "        \n",
    "    }\n",
    "\n",
    "    if df_best_models is None:\n",
    "        df_best_models = pd.DataFrame(data, index=[0])\n",
    "    else:\n",
    "        df_best_models = df_best_models.append(data, ignore_index=True)\n",
    "\n",
    "        \n",
    "print(f'Qty. of Models: {len(best_models)}')\n",
    "print('Example:')\n",
    "# display(best_models[:1][0])\n",
    "\n",
    "# display(df_best_models)\n",
    "\n",
    "csv_file = os.path.abspath(f'exec_results/best_results_by_model.csv')\n",
    "utils.save_to_csv(df=df_best_models, csv_file=csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Reexecute each best model as following:\n",
    "\n",
    " - ### $Create$ an model instance using the best hyperparameters\n",
    " - ### $Fit$ the model using the Training set\n",
    " - ### $Validate$ the model using the Validation set \n",
    " \n",
    " \n",
    "<img src=\"slides/figures/grid_search_workflow.png\" width=\"45%\" >\n",
    "\n",
    "[https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 samples were saved\n",
      "CPU times: user 9.23 s, sys: 827 ms, total: 10.1 s\n",
      "Wall time: 1.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "models_results = [] \n",
    "\n",
    "i = 1\n",
    "tot = len(best_models)\n",
    "\n",
    "\n",
    "\n",
    "## For each best model:\n",
    "#    1. Fit using using the Training data\n",
    "#    2. Validate using the Validation data\n",
    "\n",
    "for model_desc, dataset, features, clf, params, performance_grid in best_models:\n",
    "\n",
    "    # get the classifier name (without the parameters)\n",
    "    classifier_name = clf.__class__.__name__ \n",
    "\n",
    "    \n",
    "#     # Just for testing    \n",
    "#     if features != 'All Features':\n",
    "#         continue\n",
    "#     if classifier_name != 'CatBoostClassifier':\n",
    "#         continue\n",
    "#     if len(models_results) > 2:\n",
    "#         break     \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        print(f'{i}/{tot}: Executing {model_desc} using \"{dataset}\" with \"{features}\"', \n",
    "              end='\\r'\n",
    "        )\n",
    "        i += 1\n",
    "        \n",
    "        # get the correct X, y, and X_validation to be used on model execution\n",
    "        X, y, X_valid = get_X_y(dset_info=dataset, feat_info=features) \n",
    "\n",
    "        # if found the correct data\n",
    "        if X is not None:\n",
    "            # fit using the traning set\n",
    "            clf.fit(X, y)\n",
    "\n",
    "            #predict using the validation set\n",
    "            y_pred = clf.predict(X_valid)\n",
    "            \n",
    "            #get performance\n",
    "            bal_acc, sens, spec, auc, acc, prec, f1 = utils.get_scores_from_predict(\n",
    "                y_validation=y_valid, \n",
    "                y_pred=y_pred, \n",
    "            )\n",
    "            \n",
    "            # Store the Validation and Training performances\n",
    "            performance_to_save = {\n",
    "                'Model': model_desc,\n",
    "                'Dataset': dataset,\n",
    "                'Features': features,\n",
    "                # Validation performance\n",
    "                'Valid_BalAcc': bal_acc,\n",
    "                'Valid_Sens'  : sens,\n",
    "                'Valid_Spec'  : spec,\n",
    "                'Valid_f1'    : f1,\n",
    "                'Valid_AUC'   : auc,\n",
    "                'Valid_Acc'   : acc,\n",
    "                'Valid_Prec'  : prec,\n",
    "                # Voting performance\n",
    "                'Train_BalAcc': performance_grid['BalAcc'],\n",
    "                'Train_Sens'  : performance_grid['Sens'],\n",
    "                'Train_Spec'  : performance_grid['Spec'],\n",
    "                'Train_f1'    : performance_grid['f1'],\n",
    "                'Train_AUC'   : performance_grid['AUC'],\n",
    "                'Train_Acc'   : performance_grid['Acc'],\n",
    "                'Train_Prec'  : performance_grid['Prec'],\n",
    "                #\n",
    "                #\n",
    "                'Classifier': classifier_name,\n",
    "                'Hyperparams': params,\n",
    "            }\n",
    "            #\n",
    "            models_results.append(performance_to_save)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('\"X\" and \"y\" sets not found!')\n",
    "            \n",
    "            \n",
    "    except Exception as ex:\n",
    "        display(X.head(3))\n",
    "        raise ex\n",
    "        \n",
    "\n",
    "        \n",
    "clear_output(wait=False)\n",
    "       \n",
    "# create a dataFrame to store the performances and sort them\n",
    "df_validation_performance = pd.DataFrame(models_results)\n",
    "\n",
    "df_validation_performance = utils.sort_performances_results(\n",
    "    df=df_validation_performance,\n",
    "    cols_order_to_sort=['Valid_BalAcc', 'Valid_Sens', 'Valid_Spec'],\n",
    ")\n",
    "\n",
    "\n",
    "# save validation performance\n",
    "csv_file = os.path.abspath('exec_results/validation_results_for_best_models.csv')\n",
    "utils.save_to_csv(\n",
    "    df=df_validation_performance, \n",
    "    csv_file=csv_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18901"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# OTHERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
